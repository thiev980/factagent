# ğŸ” FactAgent â€“ Interaktiver Faktencheck-Assistent

Ein AI-powered Faktencheck-Tool, das Behauptungen automatisch zerlegt, im Web recherchiert, Quellen bewertet und ein strukturiertes Verdikt liefert.

**[Live Demo â†’](#)** Â· **[Portfolio â†’](#)**

![Python](https://img.shields.io/badge/Python-3.11+-blue)
![Claude API](https://img.shields.io/badge/LLM-Claude%20API-orange)
![LangGraph](https://img.shields.io/badge/Orchestration-LangGraph-green)
![Chainlit](https://img.shields.io/badge/Frontend-Chainlit-purple)

---

## Was macht FactAgent?

Du gibst eine Behauptung ein â€“ FactAgent Ã¼berprÃ¼ft sie:

```
Input:  "Die Schweiz hat die hÃ¶chste Einwanderungsrate in Europa."
Output: ğŸŸ¡ Teilweise wahr (78% Konfidenz)
        Die Schweiz hat eine der hÃ¶chsten Einwanderungsraten in Europa,
        liegt aber hinter Luxemburg und Malta...
```

## AI-Engineering-Patterns

Dieses Projekt demonstriert die folgenden AI-Engineering-Konzepte:

### 1. Agentic Workflow (ReAct Pattern)
Der Agent durchlÃ¤uft autonom 4 Schritte â€“ nicht ein einzelner API-Call, sondern eine orchestrierte Pipeline:

```
Claim Decomposer â†’ Evidence Retriever â†’ Source Evaluator â†’ Verdict Synthesizer
```

### 2. RAG (Retrieval Augmented Generation)
Das LLM recherchiert aktiv im Web, statt sich auf Trainingsdaten zu verlassen. Jede Aussage wird mit aktuellen, externen Quellen abgeglichen.

### 3. Structured Output
Jeder LLM-Call liefert validiertes JSON (via Pydantic Models) â€“ keine Freitext-Interpretation nÃ¶tig. Das ist essenziell fÃ¼r produktionsreife AI-Anwendungen.

### 4. Prompt Engineering
Jeder Agent-Schritt hat einen spezialisierten Prompt mit Rollenanweisung, Beispielen und Constraints. Prompts sind dokumentiert und versioniert.

### 5. Evaluation
Ein Eval-Set mit 15 Behauptungen (bekannte Verdikt) ermÃ¶glicht systematisches Testen und Accuracy-Tracking.

## Architektur

```
User Input (Behauptung)
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Claim Decomposer   â”‚  LLM-Call: Zerlegt in Teilaussagen
â”‚  (Claude Sonnet)     â”‚  + generiert Suchanfragen
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Evidence Retriever  â”‚  Tavily API: Web Search + Extraktion
â”‚  (Tavily Search)     â”‚  Deduplizierung, Ranking
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Source Evaluator     â”‚  LLM-Call: Relevanz, GlaubwÃ¼rdigkeit,
â”‚  (Claude Sonnet)     â”‚  Verdikt pro Teilaussage
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Verdict Synthesizer â”‚  LLM-Call: Gesamtbewertung mit
â”‚  (Claude Sonnet)     â”‚  Zusammenfassung und Quellen
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   Chat UI (Chainlit)
```

## Tech Stack

| Komponente | Technologie | Warum? |
|---|---|---|
| LLM | Claude API (Sonnet) | Starke Reasoning-FÃ¤higkeiten, gutes Preis-Leistungs-VerhÃ¤ltnis |
| Orchestrierung | LangGraph | State-Management, Routing, Error Handling |
| Web Search | Tavily API | Speziell fÃ¼r AI-Agents gebaut, hohe QualitÃ¤t |
| Structured Output | Pydantic v2 | Schema-Validierung, Typsicherheit |
| Frontend | Chainlit | Chat-UI mit Step-Visualisierung |

## Setup

### Voraussetzungen
- Python 3.11+
- Anthropic API Key ([console.anthropic.com](https://console.anthropic.com/))
- Tavily API Key ([tavily.com](https://tavily.com/) â€“ kostenloser Tier)

### Installation

```bash
# Repository klonen
git clone https://github.com/thiev980/factagent.git
cd factagent

# Virtual Environment erstellen
python -m venv venv
source venv/bin/activate  # macOS/Linux
# oder: venv\Scripts\activate  # Windows

# Dependencies installieren
pip install -r requirements.txt

# API-Keys konfigurieren
cp .env.example .env
# â†’ .env editieren und Keys eintragen
```

### Starten

```bash
# Web App starten
chainlit run app.py

# â†’ Ã–ffnet http://localhost:8000
```

### Evaluation ausfÃ¼hren

```bash
# Alle 15 Test-Claims
python -m eval.run_eval

# Nur die ersten 5
python -m eval.run_eval --limit 5
```

## Projektstruktur

```
factagent/
â”œâ”€â”€ app.py                  # Chainlit Web App (Frontend)
â”œâ”€â”€ requirements.txt        # Python Dependencies
â”œâ”€â”€ .env.example            # API-Key Template
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models.py           # Pydantic Models (Structured Output)
â”‚   â”œâ”€â”€ prompts.py          # Alle Prompts (Prompt Engineering)
â”‚   â”œâ”€â”€ tools.py            # Tavily Search (RAG)
â”‚   â”œâ”€â”€ nodes.py            # Agent-Schritte (Agentic Workflow)
â”‚   â””â”€â”€ graph.py            # LangGraph Workflow (Orchestrierung)
â”œâ”€â”€ eval/
â”‚   â”œâ”€â”€ eval_set.json       # Test-Behauptungen mit erwarteten Verdikten
â”‚   â””â”€â”€ run_eval.py         # Evaluations-Script
â””â”€â”€ README.md
```

## Verdikt-Kategorien

| Verdikt | Emoji | Bedeutung |
|---|---|---|
| `true` | âœ… | Durch Evidenz bestÃ¤tigt |
| `false` | âŒ | Durch Evidenz widerlegt |
| `partially_true` | ğŸŸ¡ | Teilweise korrekt, mit EinschrÃ¤nkungen |
| `misleading` | âš ï¸ | Technisch korrekt, aber irrefÃ¼hrend |
| `unverifiable` | â“ | Nicht genÃ¼gend verlÃ¤ssliche Quellen |

## MÃ¶gliche Erweiterungen

- **Streaming**: LLM-Antworten live streamen (Chainlit unterstÃ¼tzt das)
- **Caching**: Suchergebnisse cachen, um Kosten zu senken
- **Mehrsprachigkeit**: Automatische Spracherkennung und -anpassung
- **Source Graph**: Visualisierung der Quellen-Netzwerke
- **Human-in-the-Loop**: Nutzer kÃ¶nnen Verdikt korrigieren â†’ Feedback-Loop
- **Historische Claims**: Datenbank bereits geprÃ¼fter Behauptungen

## Kosten

Pro Faktencheck fallen ca. 3-4 Claude API Calls an (Sonnet):
- Claim Decomposition: ~500 Input + 500 Output Tokens
- Evidence Evaluation: ~2000 Input + 500 Output Tokens (pro Teilaussage)
- Verdict Synthesis: ~2000 Input + 500 Output Tokens
- Tavily: 1000 kostenlose Suchen/Monat

**GeschÃ¤tzt: ~$0.01-0.03 pro Faktencheck** (mit Claude Sonnet)

## Ãœber mich

Data Analyst mit Hintergrund in Soziologie â€“ fokussiert auf die Schnittstelle von Gesellschaft, Medien und KI. Dieses Projekt entstand als Portfolio-Showcase fÃ¼r AI Engineering.

---

*Built with Claude API, LangGraph, Tavily, and Chainlit*
